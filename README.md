# ELITE: Enhanced Language-Image Toxicity Evaluation for Safety (ICML 2025)

This is the official repository for the paper **"ELITE: Enhanced Language-Image Toxicity Evaluation for Safety"**, which has been accepted at **ICML 2025**.

[[Project Page](https://velpegor.github.io/ELITE/)] [[arXiv](https://arxiv.org/abs/2502.04757)] [[Dataset](https://huggingface.co/datasets/kdst/ELITE)]

---

## ğŸ“¢ News
* **[May 2025]** ğŸš€ Datasets are now available on Hugging Face.
* **[May 2025]** ğŸ‰ Our paper "ELITE" has been **accepted to ICML 2025**!

---

## ğŸŒŸ Overview

<p align="center">
<img src="figs/figure1.png" width=100% height=100% class="center">
</p>

Ensuring the safety of generative models requires rigorous and nuanced evaluation of toxic content across both text and image modalities. **ELITE** (Enhanced Language-Image Toxicity Evaluation) is a comprehensive safety evaluator and benchmark designed to address the limitations of existing safety assessments for generative models. By providing a more granular and trustworthy evaluation framework, ELITE enables a deeper understanding of model vulnerabilities and promotes the development of safer AI systems.

> **Core Mechanism:** ELITE introduces a sophisticated safety evaluator that analyzes complex language-image interactions to detect toxicity that traditional benchmarks might overlook.

---

## ğŸš€ Key Features

* **Safety Benchmark:** A large-scale dataset designed to test generative models against various toxicity categories.
* **Enhanced Evaluator:** A more robust evaluation metric that aligns closely with human judgment in identifying unsafe language-image pairs.

---

## ğŸ“ Citation
```bibtex
@article{lee2025elite,
  title={ELITE: Enhanced Language-Image Toxicity Evaluation for Safety},
  author={Lee, Wonjun and Lee, Doehyeon and Choi, Eugene and Yu, Sangyoon and Yousefpour, Ashkan and Park, Haon and Ham, Bumsub and Kim, Suhyun},
  journal={arXiv preprint arXiv:2502.04757},
  year={2025},
  url={[https://arxiv.org/abs/2502.04757](https://arxiv.org/abs/2502.04757)}
}
